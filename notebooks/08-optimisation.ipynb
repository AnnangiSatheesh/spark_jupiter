{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37be800-73ac-4f2f-8974-20fbe29fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# SparkSession is the entry point for the HIGH-LEVEL API (DataFrames, Spark SQL)\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"Joins\"). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5780f42-8fd4-4976-97bd-24a2f6b0639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read. \\\n",
    "    format(\"json\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    load(\"data/movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bab786-9937-4492-8424-c96285e59e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# what's wrong with a SinglePartition\n",
    "# how to add column with row_num() and count()\n",
    "# read.parquet.count use schema\n",
    "\n",
    "whole_dataset = Window.partitionBy().orderBy(col(\"Title\").asc_nulls_last())\n",
    "\n",
    "single_part_df = movies_df.select(col(\"Title\"), row_number().over(whole_dataset))\n",
    "single_part_df.explain()\n",
    "# single_part_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f09ac-1bf0-463d-a8af-a00b6d882ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_single_part_df = movies_df.select(col(\"Title\"), monotonically_increasing_id())\n",
    "non_single_part_df.explain()\n",
    "single_part_df.sample(0.1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3fa1e-e087-44d5-94b2-db93bce29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# How to read all data from cache?\n",
    "# Partial caching - cashing only parts which were calculated by some action. That is the couse that part of data\n",
    "# was from cache the other from source.\n",
    "\n",
    "partition_of_100_df = spark.range(0, 10000, 1, 100)\n",
    "partition_of_100_df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b6494-0e11-48dd-9ee7-baa64b5327f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only one partition, use only one partition FRACTION CACHE 1% - http://localhost:4040/storage/\n",
    "# consistence can be uncorrected USE .count to put all data to cache\n",
    "# deserialized - as Java object, serialized - as Array[Byte]\n",
    "\n",
    "# partition_of_100_df.show(1)\n",
    "\n",
    "partition_of_100_df.count()\n",
    "partition_of_100_df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1cc66-119e-4e53-a7c9-e4d381447d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data on local disk and disk spil\n",
    "# InMemoryRelation - load data to cache\n",
    "\n",
    "partition_of_100_df.explain()\n",
    "# InMemoryTableScn - load data to cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0d254-563e-4d1f-882a-963c4a25df4b",
   "metadata": {},
   "source": [
    "# 3 Coalesce vs repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac63f2-633e-41a5-9c41-3e312c3b3d4d",
   "metadata": {},
   "source": [
    "# 4 Join optimisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
