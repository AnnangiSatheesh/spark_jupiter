{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456e3bc-b770-4a50-b563-9163a4eca554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"Data Sources\"). \\\n",
    "    master(\"local\"). \\\n",
    "    config(\"spark.jars\", \"../jars/postgresql-42.2.19.jar\"). \\\n",
    "    config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ce3c10-b834-4e6c-b01b-5755b49f61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000), (\"John\", \"ServiceDesk\", 4600), (\"Michael\", \"Sales\", 4600), (\"Robert\", \"Sales\", 4100),\n",
    "                 (\"Maria\", \"Finance\", 3000), (\"James\", \"Sales\", 3000), (\"Scott\", \"Finance\", 3300), (\"Jen\", \"Finance\", 3900),\n",
    "                 (\"Jeff\", \"Marketing\", 3000), (\"Kumar\", \"Marketing\", 2000), (\"Saif\", \"Sales\", 4100)]\n",
    "\n",
    "employeeDF = spark.createDataFrame(simpleData).toDF(\"employee_name\", \"department\", \"salary\")\n",
    "\n",
    "employeeDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae49eb-e213-418d-81dd-00eafa77bc03",
   "metadata": {},
   "source": [
    "# Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5aeed3-06db-477b-8e55-f562a2eb0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDF.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "result_sql_df = spark.sql(\"\"\"select distinct salary from (\n",
    "                                select \n",
    "                                    employee_name, \n",
    "                                    department, \n",
    "                                    salary, \n",
    "                                    row_number() OVER (ORDER BY salary DESC) as row_num \n",
    "                              from employee) where dense_rank = 2\"\"\")\n",
    "\n",
    "result_sql_df2.explain()\n",
    "result_sql_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1242a-20d8-4c90-8868-233e1be85d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowSpec = Window.partitionBy(\"department\").orderBy(F.col(\"salary\").desc())\n",
    "windowSpec = Window.orderBy(F.col(\"salary\").desc())\n",
    "result_with_rank_df = employeeDF.\\\n",
    "    withColumn(\"rank\", F.rank().over(windowSpec)).\\\n",
    "    withColumn(\"dense_rank\", F.dense_rank().over(windowSpec))\n",
    "result_with_rank_df.explain()\n",
    "result_with_rank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7dc90-f21e-4492-bf26-d265b76da3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sql_df = spark.sql(\"\"\"\n",
    "                            select \n",
    "                                employee_name, \n",
    "                                department, \n",
    "                                salary, \n",
    "                                count(*) OVER () as cnt\n",
    "                            from employee\n",
    "                            \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adbfe5-dbe7-44a7-997a-73c093968938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Partitions Dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa40cd0-3459-4726-877b-046aef774772",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part_df_1 = employeeDF.\\\n",
    "    withColumn(\"count\", count().over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcd8f8-1b77-4ef9-ac3a-05d4bd3b2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DON'T ADD COUNT\")\n",
    "result_sql_df.show()\n",
    "result_sql_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984de0e1-f410-4182-8607-23de64542c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT WAY\n",
    "\n",
    "cnt = employeeDF.count()\n",
    "result_with_count_df = employeeDF.\\\n",
    "    withColumn(\"count\", F.lit(employeeDF.count()))\n",
    "\n",
    "result_with_count_df.show()\n",
    "result_with_count_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28d412-3a96-4ea8-879e-58d49a7f5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T ADD ROW NUM\n",
    "\n",
    "single_part_df_2 = employeeDF.\\\n",
    "    withColumn(\"row_num\", F.row_number().over(windowSpec))\n",
    "single_part_df_2.show()\n",
    "single_part_df_2.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf034da-2a5b-4c52-ab7e-34631635567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT WAY\n",
    "\n",
    "result_with_uniq_num = employeeDF.\\\n",
    "    withColumn(\"row_num\", F.monotonically_increasing_id())\n",
    "print(\"CORRECT WAY\")\n",
    "result_with_uniq_num.show()\n",
    "result_with_uniq_num.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab46d16-6742-459b-b92d-a4712d99dec8",
   "metadata": {},
   "source": [
    "# UDF, UDAF user_define_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b54e9fa-c5c2-4963-a1b7-eb4b4df94e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step-1: Define and register UDF function\n",
    "    lambda_is_world_war_two_year = lambda year: 1939 <= year <= 1945\n",
    "\n",
    "    # 1 way\n",
    "    is_world_war_two_year = udf(lambda_is_world_war_two_year)\n",
    "\n",
    "    # 2 way\n",
    "    spark.udf.register(\"isWorldWarTwoYear\", lambda_is_world_war_two_year)\n",
    "\n",
    "    stateNames = spark.read.\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferSchema\", \"true\").\\\n",
    "        csv(\"data/statenames\")\n",
    "\n",
    "    stateNames.show()\n",
    "\n",
    "    stateNames.\\\n",
    "        selectExpr(\"Year\", \"isWorldWarTwoYear(Year)\").\\\n",
    "        distinct().\\\n",
    "        show(150)\n",
    "\n",
    "    stateNames.\\\n",
    "        select(F.col(\"Year\"), is_world_war_two_year(F.col(\"Year\"))).\\\n",
    "        distinct().\\\n",
    "        show(150)\n",
    "\n",
    "    stateNames.createOrReplaceTempView(\"stateNames\")\n",
    "\n",
    "    spark.sql(\n",
    "        \"SELECT DISTINCT Name, Year FROM stateNames WHERE Year IS NOT NULL AND isWorldWarTwoYear(Year) = true ORDER BY Name DESC\").\\\n",
    "        show(150)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
