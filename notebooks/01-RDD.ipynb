{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93619a9e-3407-4736-b531-f628b35a0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53eedf-a69b-46ea-afdc-358f8b35700e",
   "metadata": {},
   "source": [
    "Spark cluster parallelism \n",
    "executors_num\n",
    "memory_per_ex\n",
    "cores_per_execut\n",
    "s = executors_num * cores_per_execut = 400 slotes\n",
    "20 block => 20 slotes ~ 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435dce30-e244-49b2-860a-46fc2f1467a7",
   "metadata": {},
   "source": [
    "# 1. HOW TO CREATE RDD\n",
    "# we can build RDDs out of local collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae08e150-e45c-4f30-8175-880e2059d4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_row(row):\n",
    "    return row.split(\",\")\n",
    "\n",
    "numbers = range(1, 1000000)\n",
    "numbers_parent_rdd = sc.parallelize(numbers, 4)\n",
    "numbers_child_rdd = numbers_parent_rdd.map(lambda x: x + 1)\n",
    "# Dependency: numbers_rdd => numbers_rdd_2\n",
    "# Linage: partition: block => numbers_rdd => numbers_rdd_2\n",
    "numbers_child_rdd.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a94ed5-704e-4588-9d53-4ac5d2d3c373",
   "metadata": {},
   "source": [
    "How to read a file in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abf843d-8b55-4198-93e0-04ffd9275e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AAPL', 'Jan 1 2000', '25.94'],\n",
       " ['AAPL', 'Feb 1 2000', '28.66'],\n",
       " ['AAPL', 'Mar 1 2000', '33.95'],\n",
       " ['AAPL', 'Apr 1 2000', '31.01'],\n",
       " ['AAPL', 'May 1 2000', '21'],\n",
       " ['AAPL', 'Jun 1 2000', '26.19'],\n",
       " ['AAPL', 'Jul 1 2000', '25.41'],\n",
       " ['AAPL', 'Aug 1 2000', '30.47'],\n",
       " ['AAPL', 'Jun 1 2004', '16.27'],\n",
       " ['AAPL', 'Jul 1 2004', '16.17']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_rdd_v2 = sc.textFile(\"data/stocks/aapl.csv\", 4). \\\n",
    "    map(lambda row: row.split(\",\")). \\\n",
    "    filter(lambda tokens: float(tokens[2]) > 15)\n",
    "\n",
    "stocks_rdd_v2.take(10)\n",
    "\n",
    "# protected def getPartitions: Array[Partition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f042d2e-0888-425c-af23-444e69ff99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from a DF\n",
    "stocks_df = spark.read.csv(\"../sources/stocks\"). \\\n",
    "    withColumnRenamed(\"_c0\", \"company\"). \\\n",
    "    withColumnRenamed(\"_c1\", \"date\"). \\\n",
    "    withColumnRenamed(\"_c2\", \"price\")\n",
    "\n",
    "stocks_rdd_v3 = stocks_df.rdd\n",
    "\n",
    "prices_rdd = stocks_rdd_v3.map(lambda row: row.price)\n",
    "prices_rdd.toDebugString()\n",
    "prices_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409efef6-edbf-46c8-986d-2358116235b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD to DF\n",
    "# condition: the RDD must contain Spark Rows (data structures conforming to a schema)\n",
    "stocks_df_v2 = spark.createDataFrame(stocks_rdd_v3)\n",
    "stocks_df_v2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33257034-53b5-464f-a6eb-1888df2f5814",
   "metadata": {},
   "source": [
    "    Use cases for RDDs\n",
    "    - the computations that cannot work on DFs/Spark SQL API\n",
    "    - very custom perf optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c67ffe-ee90-41dc-87e2-9258e7b31648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD transformations\n",
    "# map, filter, flatMap\n",
    "\n",
    "# distinct\n",
    "company_names_rdd = stocks_rdd_v3 \\\n",
    "    .map(lambda row: row.company) \\\n",
    "    .distinct()\n",
    "company_names_rdd.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09cf766-1458-4285-a664-2fe6b1feda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting\n",
    "total_entries = stocks_rdd_v3.count()  # action - the RDD must be evaluated\n",
    "print(total_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724343a-67b7-41c8-b847-072d6a4280bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max\n",
    "aapl_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .filter(lambda row: row.company == \"AAPL\") \\\n",
    "    .map(lambda row: float(row.price))\n",
    "max_aapl = aapl_stocks_rdd.max()\n",
    "print(max_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1b93c-946b-45ad-ba93-6eae9dbbcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce\n",
    "sum_prices = aapl_stocks_rdd \\\n",
    "    .reduce(lambda x, y: x + y)  # can use ANY Python function here  1,2,3,4 => 1+2 = 3 + 3 = 6 + 4\n",
    "print(sum_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce2031-9c25-407c-be5d-381f816b8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping\n",
    "grouped_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .groupBy(lambda row: row.company)  # can use ANY grouping criterion as a Python function\n",
    "# grouping is expensive - involves a shuffle\n",
    "grouped_stocks_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebaa4a8-17a9-4568-8171-c455df01f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning\n",
    "repartitioned_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .coalesce(2)\n",
    "# .repartition(30)  # involves a shuffle\n",
    "# involves a shuffle\n",
    "#  .repartition(5) 100\n",
    "#  part1 => |||||| 20           20 2  =>\n",
    "#  part2 => |||||||||||||| 40   20 2  => |||||||||||||| 40 + |||||| 20 = 60\n",
    "#  part3 => ||||| 10            20 2\n",
    "#  part4 => |||||||||| 30       20 2  => |||||||||| 30 + ||||| 10 = 40\n",
    "#  part5 =>                     20 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d5c70-c2fa-498d-8ee5-7905ed5d0fd4",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Exercises\n",
    "    1. Read the movies dataset as an RDD\n",
    "    2. Show the distinct genres as an RDD\n",
    "    3. Print all the movies in the Drama genre with IMDB rating > 6\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e63f7-5bdf-4b24-825a-0c4121a14a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.json(\"../sources/movies\")\n",
    "movies_rdd = movies_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63eaf-93ff-4e38-8277-57d64f8f883a",
   "metadata": {},
   "source": [
    "# 2. HOW TO SAVE AND PERSIST RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df60a8-27a0-4bcd-851b-ee662773780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ints = sc.parallelize(r, 4).repartition(15)\n",
    "\n",
    "ints = sc.parallelize(r).coalesce(1)\n",
    "ints.coalesce(1)\\\n",
    "    .saveAsTextFile(\"../ints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa36640-29c9-469c-aee1-24fa5fb4e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedInts = sc.textFile(\"../ints\", 4)\\\n",
    "    .map(lambda x: int(x))\\\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    .cache()\n",
    "cachedInts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44eb69-e9c8-44ca-a9bb-33c1ce12780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cachedInts.unpersist()\n",
    "\n",
    "#  very important to count() after cashing\n",
    "# cachedInts.first()\n",
    "# cachedInts.count()\n",
    "\n",
    "# cachedInts.map(lambda x: x +1).collect()\n",
    "# cachedInts.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25760d54-42e9-4272-8be7-edb4881fcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles = cachedInts.map(lambda x: x * 2)\n",
    "print(\"== Doubles\")\n",
    "for el in doubles.collect():\n",
    "    print(el)\n",
    "\n",
    "\n",
    "even = cachedInts.filter(lambda x: x % 2 == 0)\n",
    "print(\"== Even\")\n",
    "for el in even.collect():\n",
    "    print(el)\n",
    "\n",
    "even.setName(\"Even numbers\")\n",
    "print(\"Name is \" + even.name() + \" id is \" + str(even.id()))\n",
    "\n",
    "plan = even.toDebugString\n",
    "\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77352e87-0b25-4182-81f0-467331037c88",
   "metadata": {},
   "source": [
    "# 3. HOW TO GROUP AND JOIN RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4956d6-0bcd-48ca-a4ff-def362d69931",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "\n",
    "print(\"== Deduplicated\")\n",
    "# Let's calculate sum of code lines by developer\n",
    "\n",
    "reduced = codeRows.reduceByKey(lambda x, y: x + y)\n",
    "print(reduced.collect())\n",
    "deduplicated = codeRows.reduceByKey(lambda x, y: x if (x > y) else y)\n",
    "# for el in deduplicated.collect():\n",
    "#     print(el)\n",
    "\n",
    "# print()\n",
    "# print(\"== Folded\")\n",
    "# folded = codeRows.foldByKey(1000, lambda x, y: x + y)\n",
    "#\n",
    "# for el in folded.collect():\n",
    "#     print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615665f4-a0a4-4de7-9eea-1574cd829494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"== Aggregated\")\n",
    "aggregated = codeRows.aggregateByKey(500, lambda x, y: x + y, lambda x, y: x + y)\n",
    "for el in aggregated.collect():\n",
    "    print(el)\n",
    "#     part1 (k1:2, k2:2, k3:2, k1:2) shufle => (k1:2, k1:2, k1:2) => k1:6\n",
    "#     part2 (k2:2, k2:2, k3:2, k1:2) shufle => (k2:2, k2:2, k2:2) => k2:6, (k3:2, k3:2) => k3:4\n",
    "\n",
    "#     part1 (k1:2, k2:2, k3:2, k1:2) => (k1:4, k2:2, k3:2) =>  shuffle => (k1:2, k1:2, k1:2) => k1:6\n",
    "#     part2 (k2:2, k2:2, k3:2, k1:2) => (k1:4, k2:2, k3:2) => shuffle => (k2:2, k2:2, k2:2) => k2:6, (k3:2, k3:2) => k3:4\n",
    "\n",
    "#\n",
    "# # Or group items to do something else\n",
    "print()\n",
    "print(\"== Grouped\")\n",
    "grouped = codeRows.groupByKey()\n",
    "for el in grouped.collect():\n",
    "    print(str(el))\n",
    "#\n",
    "print(str(grouped.toDebugString().decode(\"utf-8\")))\n",
    "\n",
    "    # b'(1) PythonRDD[19] at collect at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:208 []\\n |\n",
    "    # MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:145 []\\n |\n",
    "    # ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "    # \\n +-(1) PairwiseRDD[16] at groupByKey at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:207 []\n",
    "    # \\n    |  PythonRDD[15] at groupByKey at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:207 []\n",
    "    # \\n    |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'\n",
    "    # # Don't forget about joins with preferred languages\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5c627-c6b9-4eb0-9419-877313514b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfced5-276d-4b22-9828-c06355168c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = programmerProfiles.join(codeRows)\n",
    "print(joined.toDebugString)\n",
    "for el in joined.collect():\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29713661-8b43-46d1-8a25-78e42fd66c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "profileData = [(\"Ivan\", \"Java\"), (\"Elena\", \"Scala\"), (\"Petr\", \"Scala\")]\n",
    "programmerProfiles = sc.parallelize(profileData)\n",
    "\n",
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "codeRows = programmerProfiles.cogroup(codeRows)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"== Cogroup\")\n",
    "cogroup_result = programmerProfiles.cogroup(codeRows).sortByKey(False).collect()\n",
    "for el in cogroup_result:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a5557-7f6b-40ae-9b76-b42bba22a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"== CountByKey\")\n",
    "print(str(joined.countByKey()))\n",
    "\n",
    "# or get all values by specific key\n",
    "print()\n",
    "print(\"== Lookup\")\n",
    "print(str(joined.lookup(\"Elena\")))\n",
    "\n",
    "# codeRows keys only\n",
    "print()\n",
    "print(\"== Keys\")\n",
    "\n",
    "for el in codeRows.keys().collect()\n",
    "\n",
    "\n",
    " # Print values only\n",
    "print(\"== Value\")\n",
    "codeRows.values.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8563ac4-6c84-4332-830f-b2563f595d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../sources/movies\")\n",
    "movies_rdd = df.rdd\n",
    "\n",
    "dist_movies = movies_rdd.map(lambda row: row.Major_Genre).distinct()\n",
    "print(dist_movies)\n",
    "\n",
    "# spark_dsl_only_df = col(\"Major_Genre\") == \"Drama\" && col(\"IMDB_Rating\") > 6\n",
    "python_lambda_rdd = lambda movie: movie.Major_Genre & movie.IMDB_Rating > 6\n",
    "\n",
    "s_movies = movies_rdd.filter(python_lambda_rdd)\n",
    "print(s_movies\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
