{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0cda5-e4ea-4c08-a0fa-fde8ff1eb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, from_json, date_format, to_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e9ec7-f46c-4aa0-b552-d12592a2b37e",
   "metadata": {},
   "source": [
    "# file_system_HDFS_S3_FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023f241-7fe2-4781-bc89-5d7149d0b3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caab552-a544-41c0-ad7c-4c2ed756c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " #    partitionBy(\"Year\"). \\\n",
    " # \\\n",
    "# Parquet = binary data, high compression, low CPU usage, very fast\n",
    "# also contains the schema\n",
    "# the default data format in Spark\n",
    "\n",
    "# stocks_df.write.save(\"data/stocks_parquet\")\n",
    "\n",
    "# each row is a value in a DF with a SINGLE column (\"value\")\n",
    "# text_df = spark.read.text(\"data/lipsum\")\n",
    "# text_df.show()\n",
    "\n",
    "# !!!!!!!!!!!!! DIFFERENCE between saveAsTable() and write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1176c-e28c-40b6-bc8d-cdb8782f7fdd",
   "metadata": {},
   "source": [
    "# data_formats_json_avro_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3c700-6c06-4585-8b40-01b2e42dfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/statenames\")\n",
    "\n",
    "state_names_df.show()\n",
    "state_names_df.printSchema()\n",
    "\n",
    "state_names_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/target/statenames_parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750ec57-33b6-43c5-9ffa-e4ae46aa9aec",
   "metadata": {},
   "source": [
    "# jdbc_postgres_oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45d80f-a814-46e3-80eb-efccf8af5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPARAMS = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "employees = \"public.employees\"\n",
    "employees_pruned = \"\"\"(select e.first_name, e.last_name, e.hire_date from public.employees e where e.gender = 'F') as new_emp\"\"\"\n",
    "\n",
    "# 10101        99999\n",
    "# 10102        99998\n",
    "# 10103        10103\n",
    "\n",
    "# df = spark.\\\n",
    "#     read.\\\n",
    "#     jdbc(url=url, table=employees, properties=DBPARAMS)\n",
    "\n",
    "# print(\"GET NUM PARTITIONS\")\n",
    "# print(df.rdd.getNumPartitions())\n",
    "\n",
    "# df.printSchema()\n",
    "# df.agg(F.max(F.col(\"emp_no\")), F.min(F.col(\"emp_no\"))).show()\n",
    "\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=\"public.employees\", properties=DBPARAMS,\n",
    "                     column=\"emp_no\", lowerBound = 10010, upperBound = 499990, numPartitions = 10)\n",
    "\n",
    "# lowerBound = 10010\n",
    "# upperBound = 499990\n",
    "#\n",
    "# ex1 => part1 => select * from public.employees e where e.emp_num > x and e.emp_num\n",
    "# ex2 => part2 =\n",
    "\n",
    "pred = [\"gender = 'F'\", \"gender = 'M'\", \"gender = 'M'\"]\n",
    "# be carefully with borders\n",
    "pred2 = [\"emp_no > 10010 and emp_no <= 50000\", \"emp_no >= 50000 and emp_no <= 100000\"]\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=\"public.employees\", properties=DBPARAMS, predicates =pred)\n",
    "df.show()\n",
    "\n",
    "# lowerBound = 10010,\n",
    "# upperBound = 499990,\n",
    "# numPartitions = 20,\n",
    "\n",
    "# Killer joins => optimised UDF\n",
    "\n",
    "# print(\"GET NUM PARTITIONS\")\n",
    "# print(df.rdd.getNumPartitions())\n",
    "#\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cd4b5-0c62-4510-8570-91dbba526437",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    option(\"driver\", driver). \\\n",
    "    option(\"url\", url). \\\n",
    "    option(\"user\", user). \\\n",
    "    option(\"password\", password). \\\n",
    "    option(\"dbtable\", \"public.employees\"). \\\n",
    "    load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd091b-9ee4-4234-a4af-2fec2d779598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# department_df = spark.read (dept_no, dept_name) // 200\n",
    "#\n",
    "# employees_df. \\\n",
    "#     groupBy(\"dept_no\"). \\\n",
    "#     count(). \\\n",
    "#     join(department_df, col(\"dept_no\") = col(\"dept_no\"),  \"inner\")\n",
    "\n",
    "# Solution1 UDF\n",
    "#\n",
    "\n",
    "print(\"GET NUM PARTITIONS\")\n",
    "print(employees_df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "employees_df.show()\n",
    "\n",
    "employees_df.write.bucketBy(10, \"emp_no\").sortBy(\"emp_no\").mode(\"overwrite\").saveAsTable(\"employee_bucketed\")\n",
    "# employees_df.write.mode(\"overwrite\").save() Parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a46c8-13c7-4aaa-bd48-2aca1dab4f1b",
   "metadata": {},
   "source": [
    "# queue_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fba04-832f-4387-a42a-85fe32edcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"page\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "# source_batch_df = spark.read\\\n",
    "#     .format(\"kafka\")\\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:29092\")\\\n",
    "#     .option(\"subscribe\", \"input\")\\\n",
    "#     .load()\n",
    "#\n",
    "# print(source_batch_df.isStreaming)\n",
    "#\n",
    "# source_batch_df.show()\n",
    "\n",
    "\n",
    "source_streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\")\\\n",
    "    .option(\"subscribe\", \"input\")\\\n",
    "    .load()\n",
    "\n",
    "print(source_streaming_df.isStreaming)\n",
    "\n",
    "typed_source_streaming_df = source_streaming_df.\\\n",
    "    select(expr(\"cast(value as string) as actualValue\")).\\\n",
    "    select(from_json(col(\"actualValue\"), schema).alias(\"page\")).\\\n",
    "    selectExpr(\"page.timestamp as timestamp\", \"page.page as page\").\\\n",
    "    select(date_format(to_timestamp(col(\"timestamp\"), \"dd-MM-yyyy HH:mm:ss:SSS\"), \"HH:mm:ss:SSS\").alias(\"time\"),col(\"page\")\n",
    "  )\n",
    "\n",
    "source_streaming_df.\\\n",
    "    writeStream.\\\n",
    "    outputMode(\"append\").\\\n",
    "    foreachBatch(lambda b, l: b.show).\\\n",
    "    trigger(processingTime='3 seconds').\\\n",
    "    start().\\\n",
    "    awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c46a91-41a2-49da-aa9b-35c110017aa7",
   "metadata": {},
   "source": [
    "Exercise: read the movies DF, then write it as\n",
    "- tab-separated \"CSV\"\n",
    "- parquet\n",
    "- table \"public.movies\" in the Postgres DB\n",
    "\n",
    "Exercise #2: find a way to read the people-1m dataFrame. Then write it as JSON."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
